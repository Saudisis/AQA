{"cells":[{"cell_type":"markdown","id":"721fcabe-c96f-417c-b6f4-ded7957081bc","metadata":{"id":"721fcabe-c96f-417c-b6f4-ded7957081bc"},"source":["# LIBRARIES SET UP"]},{"cell_type":"code","execution_count":null,"id":"a95c8686-137b-4f8b-be5b-e97c3b5c8f86","metadata":{"id":"a95c8686-137b-4f8b-be5b-e97c3b5c8f86"},"outputs":[],"source":["'''In case you encounter the error:\n","\"CRSError: Invalid projection: EPSG:4326: (Internal Proj Error: proj_create: no database context specified)\"\n","Uncomment and run the following code; then, continue.'''\n","import os\n","import pyproj\n","\n","# proj_path = r\"C:\\Users\\Administrador\\anaconda3\\envs\\cmcc_env\\Library\\share\\proj\"\n","\n","# # Force pyproj to use PROJ db\n","# os.environ[\"PROJ_LIB\"] = proj_path\n","# pyproj.datadir.set_data_dir(proj_path)\n","\n","# print(\"PROJ path set in:\", pyproj.datadir.get_data_dir())\n"]},{"cell_type":"code","source":["!pip install  cdsapi"],"metadata":{"id":"7ANn_tfMvsTa"},"id":"7ANn_tfMvsTa","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"def6c631-cc4b-4629-abb1-7eddec7a5dc2","metadata":{"cellView":"form","id":"def6c631-cc4b-4629-abb1-7eddec7a5dc2"},"outputs":[],"source":["#@markdown Imports\n","\n","import os\n","import geopandas as gpd\n","import pandas as pd\n","import numpy as np\n","import unicodedata, re\n","from shapely.geometry import box, Point\n","\n","# For Sentinel-5P access\n","\n","import ee\n","import geemap.core as geemap\n","\n","#@markdown Type in here your GEE project id\n","ee.Authenticate()\n","project_id = 'logical-lock-430313-u0' #@param {type:\"string\"}\n","ee.Initialize(project=project_id) # Change to your ProjectID\n","\n","import geemap\n","import xarray as xr # to read .nc\n","\n","from datetime import timedelta\n","from datetime import datetime\n","import time\n","from tqdm import tqdm  # for progress bar visualization\n","\n","import requests\n","from pathlib import Path\n","\n","from scipy.spatial import distance_matrix # for grid creation\n","\n","# Buttons\n","import ipywidgets as widgets\n","from IPython.display import display, clear_output\n","\n","# APIs\n","import requests\n","from sodapy import Socrata\n","from ddsapi import Client  # For CMCC DDS\n","import cdsapi\n","\n","import glob\n"]},{"cell_type":"markdown","id":"04bdae08-6fba-4aa5-ab8a-363f1d69f946","metadata":{"id":"04bdae08-6fba-4aa5-ab8a-363f1d69f946"},"source":["# CONFIGURATION SET UP"]},{"cell_type":"markdown","id":"7d249faf-c2dc-4aa7-910c-2f21db62c7be","metadata":{"id":"7d249faf-c2dc-4aa7-910c-2f21db62c7be"},"source":["To initialize the project, user must:\n","### TIME WINDOW INPUT ->\n","                        Use to know the approximate time the satellite passess over you AOI. Use as current the hour range the satellite passes, and the rest, as previous.\n","You can use the Copernicus browser: https://browser.dataspace.copernicus.eu/?zoom=5&lat=50.16282&lng=20.78613&demSource3D=%22MAPZEN%22&cloudCoverage=30&dateMode=SINGLE.\n","If you click on “Find products for current view”, and then click on “product info”, then you can see the sensing time in the pop up.\n","\n","### BOUNDING BOX INPUT ->\n","                        Use to limit your Area Of Interest\n","Suggested:\n","\n","| Province             | Min Lon | Min Lat | Max Lon | Max Lat |\n","| -------------------- | ------- | ------- | ------- | ------- |\n","| **Milan (Milano)**   | 8.9     | 45.3    | 9.6     | 45.7    |\n","| **Monza e Brianza**  | 9.1     | 45.55   | 9.45    | 45.75   |\n","| **Varese**           | 8.55    | 45.65   | 8.95    | 46.15   |\n","| **Como**             | 8.85    | 45.70   | 9.30    | 46.20   |\n","| **Lecco**            | 9.25    | 45.75   | 9.60    | 46.15   |\n","| **Bergamo**          | 9.45    | 45.55   | 10.15   | 46.10   |\n","| **Brescia**          | 9.85    | 45.25   | 10.70   | 46.25   |\n","| **Pavia**            | 8.70    | 44.80   | 9.35    | 45.25   |\n","| **Lodi**             | 9.25    | 45.15   | 9.65    | 45.45   |\n","| **Cremona**          | 9.55    | 45.00   | 10.25   | 45.45   |\n","| **Mantova (Mantua)** | 10.30   | 44.90   | 11.00   | 45.35   |\n","| **Sondrio**          | 9.25    | 46.05   | 10.40   | 46.65   |\n"]},{"cell_type":"code","execution_count":null,"id":"0f75fec1-928c-4bff-8cd2-540add29a1c5","metadata":{"jupyter":{"source_hidden":true},"cellView":"form","id":"0f75fec1-928c-4bff-8cd2-540add29a1c5"},"outputs":[],"source":["# @markdown Configure\n","#  Initialize empty cfg\n","cfg = {}\n","\n","#  Widgets for BOUNDING BOX\n","lon_min = widgets.FloatText(value=8.9, description='Min Lon:')\n","lat_min = widgets.FloatText(value=45.3, description='Min Lat:')\n","lon_max = widgets.FloatText(value=9.6, description='Max Lon:')\n","lat_max = widgets.FloatText(value=45.7, description='Max Lat:')\n","\n","#  Widgets for TIME WINDOWS\n","prev_start = widgets.Text(value='15:01', description='Prev start:')\n","prev_end = widgets.Text(value='11:59', description='Prev end:')\n","curr_start = widgets.Text(value='12:00', description='Curr start:')\n","curr_end = widgets.Text(value='15:00', description='Curr end:')\n","\n","# Button to apply configuration\n","button = widgets.Button(description=\"Generate cfg\", button_style='success')\n","output = widgets.Output()\n","\n","def on_button_clicked(b):\n","    global cfg\n","    with output:\n","        clear_output()\n","        cfg = {\n","            \"bbox\": [lon_min.value, lat_min.value, lon_max.value, lat_max.value],\n","            \"crs_grid\": \"EPSG:32632\",\n","            \"crs_gee\": \"EPSG:4326\",\n","            \"sentinel_scale_km\": 5.5,\n","            \"arpa_resolution_deg\": 0.005,\n","            \"start_date\": None,\n","            \"end_date\": None,\n","            \"time_windows\": {\n","                \"prev\": (prev_start.value, prev_end.value),\n","                \"curr\": (curr_start.value, curr_end.value)\n","            }\n","        }\n","\n","        # Construct AOI GeoDataFrame\n","        minx, miny, maxx, maxy = cfg[\"bbox\"]\n","        aoi = gpd.GeoDataFrame(\n","            {\"id\": [1]},\n","            geometry=[box(minx, miny, maxx, maxy)],\n","            crs=cfg[\"crs_gee\"]\n","        )\n","        aoi_utm = aoi.to_crs(cfg[\"crs_grid\"])\n","        aoi_gee = ee.Geometry.Rectangle([minx, miny, maxx, maxy])\n","\n","        cfg[\"aoi\"] = aoi\n","        cfg[\"aoi_utm\"] = aoi_utm\n","        cfg[\"aoi_gee\"] = aoi_gee\n","\n","        print(\"Configuration saved to variable `cfg`. Continue\")\n","        #print(\"Current cfg\")\n","        #print(cfg)\n","\n","button.on_click(on_button_clicked)\n","\n","# Display interface\n","print(\"Adjust bounding box and time windows, then click 'Generate cfg':\")\n","bbox_box = widgets.VBox([lon_min, lat_min, lon_max, lat_max])\n","time_box = widgets.VBox([prev_start, prev_end, curr_start, curr_end])\n","ui = widgets.HBox([bbox_box, time_box])\n","\n","display(ui, button, output)\n"]},{"cell_type":"code","execution_count":null,"id":"c56d6370-7dc8-4a65-b84a-90223d60656f","metadata":{"cellView":"form","id":"c56d6370-7dc8-4a65-b84a-90223d60656f"},"outputs":[],"source":["# @markdown Select time period\n","start_date_widget = widgets.DatePicker(description=\"Start Date:\")\n","end_date_widget = widgets.DatePicker(description=\"End Date:\")\n","save_button = widgets.Button(description=\"Save Dates\", button_style=\"success\")\n","output_range = widgets.Output()\n","\n","display(start_date_widget, end_date_widget, save_button, output_range)\n","\n","def save_dates(b):\n","    with output_range:\n","        output_range.clear_output()\n","\n","        # Save in global variables and cfg\n","        global start_date, end_date\n","        start_date = pd.to_datetime(start_date_widget.value)\n","        end_date = pd.to_datetime(end_date_widget.value)\n","\n","        cfg[\"start_date\"] = start_date\n","        cfg[\"end_date\"] = end_date\n","\n","        print(f\" Dates have been saved:\")\n","        print(f\" Start: {start_date.date()}\")\n","        print(f\" End: {end_date.date()}\")\n","        print(f\" Please, continue\")\n","\n","save_button.on_click(save_dates)"]},{"cell_type":"code","execution_count":null,"id":"ebcfdd73-1c2d-4301-89a4-8136d8a4e2db","metadata":{"cellView":"form","id":"ebcfdd73-1c2d-4301-89a4-8136d8a4e2db"},"outputs":[],"source":["#@markdown Date Range Definition\n","if cfg[\"start_date\"] is not None and cfg[\"end_date\"] is not None:\n","    dates = pd.date_range(cfg[\"start_date\"], cfg[\"end_date\"], freq=\"D\")\n","    #print(\"Dates:\", dates[:10], \"...\")\n","else:\n","    print(\"Date Range not yet defined.\")\n","\n","dates = pd.date_range(cfg[\"start_date\"], cfg[\"end_date\"], freq=\"D\")\n","print(\"Process through\", [d.date() for d in dates])"]},{"cell_type":"markdown","id":"994e9255-f438-42a8-94b1-8b54f500747d","metadata":{"id":"994e9255-f438-42a8-94b1-8b54f500747d"},"source":["# ARPA"]},{"cell_type":"markdown","id":"7e9818ca-6be3-4d9d-82bf-4b14be17e7db","metadata":{"id":"7e9818ca-6be3-4d9d-82bf-4b14be17e7db"},"source":["##### In order to download ARPA data, you must have an account at: https://www.dati.lombardia.it/login\n","##### Once you register, you will have access to an App Token that can be found here: https://www.dati.lombardia.it/profile/edit/developer_settings\n","##### Please copy your token for the app_token variable.\n","##### *Note: ARPAs' creation date was novemeber 20, 2017. Data prior to that date is not available."]},{"cell_type":"markdown","id":"abf64717-b073-40d6-9664-dd79d8ab43fa","metadata":{"id":"abf64717-b073-40d6-9664-dd79d8ab43fa"},"source":["### ARPA Lombardia Data Download and Analysis"]},{"cell_type":"code","execution_count":null,"id":"dfb200cd-c4f3-4f4b-92b1-6b3a3a53b6c8","metadata":{"cellView":"form","id":"dfb200cd-c4f3-4f4b-92b1-6b3a3a53b6c8"},"outputs":[],"source":["#@markdown ARPA Lombardia Data Download and Analysis\n","# 1. Configuration\n","app_token = \"vgiEMaAWkrTroWwi42G71LlIS\"\n","headers = {\"X-App-Token\": app_token}\n","\n","# Endpoints (main + fallback)\n","data_url = \"https://www.dati.lombardia.it/resource/nicp-bhqi.json\"   # Main endpoint\n","backup_url = \"https://www.dati.lombardia.it/resource/g2hp-ar79.json\"  # Fallback endpoint\n","\n","# Metadata URL\n","meta_url = \"https://www.dati.lombardia.it/resource/ib47-atvt.csv?$limit=50000\"\n","\n","# 2. Metadata Loading\n","meta = (\n","    pd.read_csv(meta_url)\n","    .dropna(subset=[\"idsensore\", \"nometiposensore\", \"lat\", \"lng\", \"provincia\"])\n","    .assign(\n","        idsensore=lambda df: df[\"idsensore\"].astype(int),\n","        provincia=lambda df: df[\"provincia\"].str.upper()\n","    )\n",")\n","provinces = sorted(meta[\"provincia\"].unique())\n","pollutants = sorted(meta[\"nometiposensore\"].unique())\n","\n","province = widgets.Dropdown(options=provinces, description=\"Province:\")\n","pollutant = widgets.Dropdown(options=pollutants, description=\"Pollutant:\")\n","display(province, pollutant)\n","\n","# 3. Download Function\n","def download_data(province, pollutant):\n","    meta_f = meta[\n","        (meta[\"provincia\"] == province.upper()) &\n","        (meta[\"nometiposensore\"] == pollutant)\n","    ]\n","    if meta_f.empty:\n","        print(f\"No sensors for '{pollutant}' in '{province}'\")\n","        return None\n","    print(f\"\\nSensors found in {province} for {pollutant}: {len(meta_f)}.\")\n","    print(f\"\\nPlease wait until message of complete successful download appears, only then, click Analyze.\")\n","\n","    ids = meta_f[\"idsensore\"].tolist()\n","    all_data = []\n","\n","    if cfg[\"start_date\"] is None or cfg[\"end_date\"] is None:\n","        print(\"Please define start_date and end_date before downloading.\")\n","        return None\n","\n","    # Iterate through date range\n","    for date_analysis in pd.date_range(cfg[\"start_date\"], cfg[\"end_date\"], freq=\"D\"):\n","        y, m, d = date_analysis.year, date_analysis.month, date_analysis.day\n","        print(f\"\\nDownloading data: {pollutant} ({province}) for {date_analysis.date()}\")\n","\n","        params = {\n","            \"$select\": \"idsensore,data,valore\",\n","            \"$where\": f\"date_extract_y(data)={y} AND date_extract_m(data)={m} AND date_extract_d(data)={d}\",\n","            \"$limit\": 50000\n","        }\n","\n","        # Try both endpoints sequentially\n","        urls_to_try = [data_url, backup_url]\n","\n","        data = pd.DataFrame()\n","        for url in urls_to_try:\n","            try:\n","                print(f\"→ Trying source: {url.split('/')[-1]} ...\")\n","                r = requests.get(url, headers=headers, params=params)\n","                r.raise_for_status()\n","                temp = pd.DataFrame(r.json())\n","                if not temp.empty:\n","                    data = temp\n","                    print(f\"Data found in {url.split('/')[-1]} ({len(data)} records)\")\n","                    break\n","                else:\n","                    print(\"No data in this source, trying next\")\n","            except Exception as e:\n","                print(f\"Error with {url.split('/')[-1]}: {e}\")\n","\n","        if data.empty:\n","            print(\"No information available for this date in any source.\")\n","            continue\n","\n","        # Convert types\n","        data[\"idsensore\"] = pd.to_numeric(data[\"idsensore\"], errors=\"coerce\").astype(\"Int64\")\n","        data[\"data\"] = pd.to_datetime(data[\"data\"], errors=\"coerce\")\n","        data[\"valore\"] = pd.to_numeric(data[\"valore\"], errors=\"coerce\")\n","        data = data[data[\"idsensore\"].isin(ids)]\n","\n","        # Merge ARPA with Metadata\n","        merged = data.merge(meta_f, on=\"idsensore\", how=\"inner\")\n","        merged = merged.rename(columns={\"nometiposensore\": \"pollutant\", \"valore\": \"value\"})\n","        merged = merged[[\"idsensore\", \"pollutant\", \"data\", \"value\", \"lat\", \"lng\", \"provincia\"]]\n","        merged[\"date\"] = date_analysis.date()\n","\n","        all_data.append(merged)\n","        print(f\"{len(merged)} records downloaded for {date_analysis.date()}\")\n","\n","    if all_data:\n","        merged_all = pd.concat(all_data, ignore_index=True)\n","        print(f\"\\nCombined {len(all_data)} days of data ({cfg['start_date'].date()} - {cfg['end_date'].date()})\")\n","        return merged_all\n","    else:\n","        print(\"No data downloaded in the selected range.\")\n","        return None\n","\n","# 4. Data Analysis\n","def analyze_data(merged, pollutant, province):\n","    if merged is None or merged.empty:\n","        print(\"No data to analyze.\")\n","        return None\n","\n","    if cfg[\"start_date\"] is None or cfg[\"end_date\"] is None:\n","        print(\"Date range hasn't been defined.\")\n","        return None\n","\n","    all_summaries = []\n","\n","    for date_analysis in pd.date_range(cfg[\"start_date\"], cfg[\"end_date\"], freq=\"D\"):\n","        prev_date = date_analysis - pd.Timedelta(days=1)\n","        subset = merged[merged[\"data\"].dt.date.isin([prev_date.date(), date_analysis.date()])].copy()\n","\n","        if subset.empty:\n","            print(f\"No data for {date_analysis.date()}\")\n","            continue\n","\n","        subset[\"hour\"] = subset[\"data\"].dt.hour\n","\n","        def classify_period(row):\n","            date, hour = row[\"data\"].date(), row[\"hour\"]\n","            if date == date_analysis.date() and 12 <= hour < 15:\n","                return \"current\"\n","            elif (date == prev_date.date() and hour >= 15) or (date == date_analysis.date() and hour < 12):\n","                return \"previous\"\n","            else:\n","                return None\n","\n","        subset[\"period\"] = subset.apply(classify_period, axis=1)\n","        subset = subset.dropna(subset=[\"period\"])\n","\n","        # Mean Computation\n","        summary = (\n","            subset.groupby([\"idsensore\", \"lat\", \"lng\", \"provincia\", \"period\"])[\"value\"]\n","            .mean()\n","            .reset_index()\n","            .pivot(index=[\"idsensore\", \"lat\", \"lng\", \"provincia\"],\n","                   columns=\"period\",\n","                   values=\"value\")\n","            .reset_index()\n","            .rename(columns={\"current\": \"curr_mean\", \"previous\": \"prev_mean\"})\n","        )\n","\n","        summary[\"pollutant\"] = pollutant\n","        summary[\"date\"] = date_analysis.date()\n","        all_summaries.append(summary)\n","        print(f\"Date analyzed: {date_analysis.date()} — Province: {province} — Pollutant: {pollutant}\")\n","\n","    if all_summaries:\n","        summary_all = pd.concat(all_summaries, ignore_index=True)\n","        print(f\"\\nAnalysis completed for {len(all_summaries)} days ({cfg['start_date'].date()} → {cfg['end_date'].date()})\")\n","        display(summary_all.head())\n","        return summary_all\n","    else:\n","        print(\"No summaries generated.\")\n","        return None\n","\n","# 5. Interface (Widgets)\n","button_download = widgets.Button(description=\"Download Data\", button_style=\"success\")\n","button_analyze = widgets.Button(description=\"Analyze\", button_style=\"info\")\n","output = widgets.Output()\n","\n","state = {\"merged\": None, \"province\": None, \"pollutant\": None}\n","\n","def on_download(b):\n","    with output:\n","        output.clear_output()\n","        state[\"province\"] = province.value\n","        state[\"pollutant\"] = pollutant.value\n","\n","        merged = download_data(state[\"province\"], state[\"pollutant\"])\n","        state[\"merged\"] = merged\n","\n","        # Save filtered metadata\n","        state[\"meta_f\"] = meta[\n","            (meta[\"provincia\"].str.upper() == state[\"province\"].upper()) &\n","            (meta[\"nometiposensore\"] == state[\"pollutant\"])\n","        ]\n","\n","        if merged is not None and not merged.empty:\n","            print(\"\\nData and metadata loaded successfully. Click Analyze to proceed\")\n","        else:\n","            print(\"No data was downloaded. Retry.\")\n","\n","def on_analyze(b):\n","    with output:\n","        output.clear_output()\n","        gdf = analyze_data(state[\"merged\"], state[\"pollutant\"], state[\"province\"])\n","        if gdf is not None:\n","            state[\"summary\"] = gdf\n","            print(\"GeoDataFrame created successfully. You can continue\")\n","\n","button_download.on_click(on_download)\n","button_analyze.on_click(on_analyze)\n","\n","print(\"\\nFIRST: Choose the Province and Pollutant of Interest and click Download Data\\n\")\n","display(button_download, button_analyze, output)\n"]},{"cell_type":"markdown","id":"da80a96d-cd1e-458b-9ae8-05673d2f7de3","metadata":{"id":"da80a96d-cd1e-458b-9ae8-05673d2f7de3"},"source":["###  ARPA Grid Generation"]},{"cell_type":"code","execution_count":null,"id":"8ffb025e-75ea-4c74-9446-35842d12ae8b","metadata":{"cellView":"form","id":"8ffb025e-75ea-4c74-9446-35842d12ae8b"},"outputs":[],"source":["#@markdown Grid Generation\n","def build_grid(meta_f, pollutant):\n","\n","    # Sensors GeoDataFrame\n","    sensors_gdf = gpd.GeoDataFrame(\n","        meta_f,\n","        geometry=gpd.points_from_xy(meta_f[\"lng\"], meta_f[\"lat\"]),\n","        crs=\"EPSG:4326\"\n","    )\n","\n","    # Compute Distance Matrix (degrees)\n","    coords = np.array(list(zip(sensors_gdf.geometry.x, sensors_gdf.geometry.y)))\n","    if len(coords) < 2:\n","        print(\"Cannot compute grid: only one sensor available.\")\n","        return None\n","\n","    dist_mat = distance_matrix(coords, coords)\n","    dist_mat[dist_mat == 0] = np.nan  # avoid zero distances\n","\n","    min_dist_deg = np.nanmin(dist_mat)\n","    if np.isnan(min_dist_deg) or min_dist_deg == 0:\n","        print(\"Cannot compute grid: invalid or identical coordinates.\")\n","        return None\n","\n","    print(f\"\\nMinimum distance between sensors ({pollutant}): {min_dist_deg:.6f} degrees\")\n","\n","    # Grid Resolution\n","    # Use 90% of minimum distance to make grid slightly denser than sensor spacing\n","    res = min_dist_deg * 0.9\n","\n","    # Grid Creation within bounding box\n","    minx, miny, maxx, maxy = sensors_gdf.total_bounds\n","    xs = np.arange(minx, maxx, res)\n","    ys = np.arange(miny, maxy, res)\n","\n","    grid_points = [Point(x, y) for x in xs for y in ys]\n","    grid_gdf = gpd.GeoDataFrame(\n","        {\"grid_id\": range(len(grid_points))},\n","        geometry=grid_points,\n","        crs=\"EPSG:4326\"\n","    )\n","\n","    print(f\"\\nGrid generated for {pollutant}: {len(grid_gdf)} points — resolution ≈ {res:.6f}°\")\n","    display(grid_gdf.head())\n","\n","    return grid_gdf\n","\n","\n","meta_f = state[\"meta_f\"]\n","pollutant_name = state[\"pollutant\"]\n","\n","print(\"\\nSelected sensors for grid generation:\")\n","display(meta_f[[\"idsensore\", \"nometiposensore\", \"lat\", \"lng\"]])\n","print(f\"Total sensors: {len(meta_f)}\")\n","\n","# Build grid\n","grid_gdf = build_grid(meta_f, pollutant_name)\n","\n","# Save in state for later ERA5 interpolation\n","state[\"grid_gdf\"] = grid_gdf\n"]},{"cell_type":"markdown","id":"9e543473-667b-415c-bde4-8bb15720dae5","metadata":{"id":"9e543473-667b-415c-bde4-8bb15720dae5"},"source":["# ERA5"]},{"cell_type":"markdown","id":"beb89381-5a6b-442f-a7fb-1961796ecd0b","metadata":{"id":"beb89381-5a6b-442f-a7fb-1961796ecd0b"},"source":["## ACCESS TO ERA5"]},{"cell_type":"markdown","id":"dc22cf44-94da-4888-9aa4-078eb455c321","metadata":{"id":"dc22cf44-94da-4888-9aa4-078eb455c321"},"source":["##### Before accessing ERA5, please read and setup de CDSAPI.\n","##### You can follow the instructions from this webpage: https://cds.climate.copernicus.eu/how-to-api"]},{"cell_type":"markdown","id":"38450418-52ec-4f51-a6ac-0ecb0886fc7c","metadata":{"id":"38450418-52ec-4f51-a6ac-0ecb0886fc7c"},"source":["### API fetch - ERA5 Download"]},{"cell_type":"code","execution_count":null,"id":"77f89aa5-b3f7-4fd8-ada9-75e8695f2901","metadata":{"id":"77f89aa5-b3f7-4fd8-ada9-75e8695f2901"},"outputs":[],"source":["# Clean previous downloads\n","for f in glob.glob(\"era5_data/*.nc\"):\n","    os.remove(f)\n","print(\"Removed previous ERA5 files (if any). The folder is ready for new downloads.\")\n","\n","# Initialize CDS client\n","client = cdsapi.Client()\n","os.makedirs(\"era5_data\", exist_ok=True)\n","\n","start_date = pd.to_datetime(cfg[\"start_date\"])\n","end_date = pd.to_datetime(cfg[\"end_date\"])\n","\n","if start_date > end_date:\n","    print(\"❌ Invalid date range: start_date is after end_date.\")\n","else:\n","    bbox = [7, 44, 10, 48]\n","    area = [bbox[3], bbox[0], bbox[1], bbox[2]]\n","\n","    variables = [\n","        \"2m_temperature\",\n","        \"surface_net_solar_radiation\",\n","        \"surface_net_thermal_radiation\",\n","        \"surface_pressure\",\n","        \"total_precipitation\",\n","        \"10m_u_component_of_wind\",\n","        \"10m_v_component_of_wind\",\n","        \"boundary_layer_height\"\n","    ]\n","\n","    for var in variables:\n","        out_file = f\"era5_data/{var}_{start_date.date()}_{end_date.date()}.nc\"\n","        if os.path.exists(out_file):\n","            print(f\"✅ {var} already exists — skipping download.\")\n","            continue\n","\n","        try:\n","            print(f\"⬇️ Downloading {var} ({start_date.date()} → {end_date.date()})...\")\n","            client.retrieve(\n","                \"reanalysis-era5-single-levels\",\n","                {\n","                    \"product_type\": \"reanalysis\",\n","                    \"format\": \"netcdf\",\n","                    \"variable\": [var],\n","                    \"date\": f\"{start_date.date()}/{end_date.date()}\",\n","                    \"time\": [f\"{h:02d}:00\" for h in range(24)],\n","                    \"area\": area\n","                },\n","                out_file\n","            )\n","            print(f\"✅ Saved: {out_file}\")\n","\n","        except Exception as e:\n","            print(f\"❌ Error downloading {var}: {e}\")\n","\n","    print(\"\\n✅ All downloads completed. Files saved in /era5_data/\")\n"]},{"cell_type":"markdown","id":"6c74a4c4-e59d-4037-af2b-cd401016b001","metadata":{"id":"6c74a4c4-e59d-4037-af2b-cd401016b001"},"source":["### One dataset with all variables:"]},{"cell_type":"code","execution_count":null,"id":"0e88d271-a291-48c7-b9be-9f3346211adc","metadata":{"id":"0e88d271-a291-48c7-b9be-9f3346211adc"},"outputs":[],"source":["# Merge all .nc files into one data set\n","folder = \"era5_data\"\n","files = sorted(glob.glob(os.path.join(folder, \"*.nc\")))\n","datasets = [xr.open_dataset(f) for f in files]\n","merged_xr = xr.merge(datasets)   # xarray copy if future need\n","\n","# Convert to Dataframe\n","ds_merged = merged_xr.to_dataframe().reset_index()\n","\n","print(\"Merged Dataset\")\n","display(ds_merged.head())"]},{"cell_type":"markdown","id":"3b4f5ec2-0c1b-4d68-a24d-0cae1de7d2c2","metadata":{"id":"3b4f5ec2-0c1b-4d68-a24d-0cae1de7d2c2"},"source":["### Units Conversion"]},{"cell_type":"code","execution_count":null,"id":"e461cf3b-957a-442f-95db-f3c54ef995ff","metadata":{"id":"e461cf3b-957a-442f-95db-f3c54ef995ff"},"outputs":[],"source":["# Units Conversion\n","\n","# Temperature from [K to °C]\n","if \"t2m\" in ds_merged:\n","    ds_merged[\"t2m_c\"] = ds_merged[\"t2m\"] - 273.15\n","\n","# Pressure from [Pa to hPa]\n","if \"sp\" in ds_merged:\n","    ds_merged[\"sp_hpa\"] = ds_merged[\"sp\"] / 100.0\n","\n","# Surface Solar Radiation from [J to kJ]\n","if \"ssr\" in ds_merged:\n","    ds_merged[\"ssr_kJ\"] = ds_merged[\"ssr\"] / 1000.0\n","\n","# Surface Thermal Radiation from [J to kJ]\n","if \"str\" in ds_merged:\n","    ds_merged[\"str_kJ\"] = ds_merged[\"str\"] / 1000.0\n","\n","# Boundary Layer Height [m to km]\n","if \"blh\" in ds_merged:\n","    ds_merged[\"blh_km\"] = ds_merged[\"blh\"] / 1000.0\n","\n","# Total windspeed computation [m/s] (using u10 and v10)\n","if all(v in ds_merged for v in [\"u10\", \"v10\"]):\n","    ds_merged[\"wind_speed\"] = np.sqrt(ds_merged[\"u10\"]**2 + ds_merged[\"v10\"]**2)\n","\n","#display(ds_merged.head(20))"]},{"cell_type":"markdown","id":"c12aa818-67ae-4d31-a0a6-5399d62f99c7","metadata":{"id":"c12aa818-67ae-4d31-a0a6-5399d62f99c7"},"source":["### Grid Alignment for future interpolation"]},{"cell_type":"code","execution_count":null,"id":"f91432e6-691e-4850-b17f-53c490ddd769","metadata":{"id":"f91432e6-691e-4850-b17f-53c490ddd769"},"outputs":[],"source":["# Grid alignment\n","\n","grid_wgs84 = grid_gdf.to_crs(\"EPSG:4326\")\n","grid_wgs84[\"lat\"] = grid_wgs84.geometry.y\n","grid_wgs84[\"lon\"] = grid_wgs84.geometry.x"]},{"cell_type":"markdown","id":"9197c109-2c1e-4869-a1fd-da731600f9cf","metadata":{"id":"9197c109-2c1e-4869-a1fd-da731600f9cf"},"source":["### ERA5 + ARPA variables interpolation process"]},{"cell_type":"code","execution_count":null,"id":"b03cd9ca-40c0-4a97-9ff9-16e4c8a50b80","metadata":{"id":"b03cd9ca-40c0-4a97-9ff9-16e4c8a50b80"},"outputs":[],"source":["# Interpolated variables to join with ARPA\n","\n","# DataFrame to xarray Dataset\n","ds_xr = ds_merged.set_index([\"valid_time\", \"latitude\", \"longitude\"]).to_xarray()\n","\n","vars_era5 = [\"t2m_c\", \"sp_hpa\", \"wind_speed\", \"blh_km\", \"tp\", \"ssr_kJ\", \"str_kJ\"]\n","results = []\n","\n","for t in tqdm(ds_xr[\"valid_time\"].values, desc=\"Interpolating ERA5 on ARPA grid\"):\n","    ds_hour = ds_xr.sel(valid_time=t)\n","\n","    # Bilinear interpolation on the grid\n","    interp = ds_hour[vars_era5].interp(\n","        latitude=(\"points\", grid_wgs84[\"lat\"]),\n","        longitude=(\"points\", grid_wgs84[\"lon\"])\n","    )\n","\n","# To DataFrame\n","    interp_df = interp.to_dataframe().reset_index(drop=True)\n","    interp_df[\"valid_time\"] = pd.to_datetime(t)\n","\n","# Grid Coordinates\n","    interp_df[\"grid_id\"] = grid_wgs84[\"grid_id\"]\n","    interp_df[\"lat\"] = grid_wgs84[\"lat\"]\n","    interp_df[\"lon\"] = grid_wgs84[\"lon\"]\n","\n","    results.append(interp_df)\n","\n","# Concatenate\n","era5_on_arpa = pd.concat(results, ignore_index=True)\n","\n","print(era5_on_arpa.head())\n","print(f\"\\nConcatenated number of rows: {len(era5_on_arpa)}\")"]},{"cell_type":"markdown","id":"d8dbbc7a-a1c7-4972-9b3b-5e10e4605b69","metadata":{"id":"d8dbbc7a-a1c7-4972-9b3b-5e10e4605b69"},"source":["### ERA5 mean computation"]},{"cell_type":"code","execution_count":null,"id":"04349086-569a-4524-8f60-86a6991a0cf4","metadata":{"id":"04349086-569a-4524-8f60-86a6991a0cf4"},"outputs":[],"source":["# 1. ERA5 cleaning\n","era5_df = era5_on_arpa.drop(columns=[\"lat\", \"lon\"], errors=\"ignore\").copy()\n","\n","# 2. Period time defined in cfg\n","def assign_period(df, date):\n","    curr_start = pd.to_datetime(f\"{date} {cfg['time_windows']['curr'][0]}\")\n","    curr_end   = pd.to_datetime(f\"{date} {cfg['time_windows']['curr'][1]}\")\n","    prev_start = pd.to_datetime(f\"{(pd.to_datetime(date) - pd.Timedelta(days=1)).date()} {cfg['time_windows']['prev'][0]}\")\n","    prev_end   = pd.to_datetime(f\"{date} {cfg['time_windows']['prev'][1]}\")\n","\n","    def classify(ts):\n","        if curr_start <= ts <= curr_end:\n","            return \"current\"\n","        elif prev_start <= ts <= prev_end:\n","            return \"previous\"\n","        else:\n","            return None\n","\n","    df[\"period\"] = df[\"valid_time\"].apply(classify)\n","    return df.dropna(subset=[\"period\"])\n","\n","\n","for date_analysis in pd.date_range(cfg[\"start_date\"], cfg[\"end_date\"], freq=\"D\"):\n","    print(f\"\\nProcessing ERA5 for {date_analysis.date()}\")\n","    era5_df_day = assign_period(era5_df, date_analysis.date())\n","\n","    era5_summary = (\n","        era5_df_day.groupby([\"grid_id\", \"period\"], as_index=False)\n","        .mean(numeric_only=True)\n","        .pivot(index=\"grid_id\", columns=\"period\")\n","        .reset_index()\n","    )\n","    print(f\"\\nGenerated rows for {date_analysis.date()}: {len(era5_summary)}\")\n","\n","# 3. Mean Computation\n","era5_summary = (\n","    era5_df.groupby([\"grid_id\", \"period\"], as_index=False)\n","    .mean(numeric_only=True)\n","    .pivot(index=\"grid_id\", columns=\"period\")\n","    .reset_index()\n",")\n","\n","# Flatten MultiIndex\n","era5_summary.columns = [\n","    \"_\".join([str(c) for c in col if c not in (\"\", None)]).strip(\"_\")\n","    if isinstance(col, tuple) else str(col)\n","    for col in era5_summary.columns\n","]\n","\n","# 4. Removal of redundant coordinate columns\n","era5_summary = era5_summary.loc[:, ~era5_summary.columns.str.contains(\"lat|lon|latitude|longitude\", case=False)]\n","era5_summary = era5_summary.drop(columns=[\"number_current\", \"number_previous\"], errors=\"ignore\")\n","\n","# 5. Merge with grid coordinates\n","era5_summary = era5_summary.merge(\n","    grid_wgs84[[\"grid_id\", \"lat\", \"lon\"]],\n","    on=\"grid_id\",\n","    how=\"left\"\n",")\n","\n","# 6. Reordering for readability\n","ordered_cols = [\"grid_id\", \"lat\", \"lon\"] + sorted(\n","    [c for c in era5_summary.columns if any(x in c for x in [\"_current\", \"_previous\"])]\n",")\n","era5_summary = era5_summary[ordered_cols]\n","\n","# Summary preview\n","print(f\"\\nERA5 summary for {date_analysis.date()} ({pollutant.value}, {province.value})\\n\")\n","display(era5_summary.filter(regex=\"t2m|sp|wind|ssr|str|blh|tp|lat|lon\").head())"]},{"cell_type":"markdown","id":"c2fbe1de-132e-4336-9bb7-11407091e514","metadata":{"id":"c2fbe1de-132e-4336-9bb7-11407091e514"},"source":["# ARPA + ERA5"]},{"cell_type":"code","execution_count":null,"id":"25a420b0-3891-441b-99ec-9c770aa7181e","metadata":{"id":"25a420b0-3891-441b-99ec-9c770aa7181e"},"outputs":[],"source":["# ERA5 summary to GeoDataFrame\n","summary = state[\"summary\"]\n","era5_gdf = gpd.GeoDataFrame(\n","    era5_summary,\n","    geometry=gpd.points_from_xy(era5_summary[\"lon\"], era5_summary[\"lat\"]),\n","    crs=\"EPSG:4326\"\n",").to_crs(\"EPSG:32632\")\n","\n","# ARPA sensors (summary) to GeoDataFrame\n","summary_gdf = gpd.GeoDataFrame(\n","    summary,\n","    geometry=gpd.points_from_xy(summary[\"lng\"], summary[\"lat\"]),\n","    crs=\"EPSG:4326\"\n",").to_crs(\"EPSG:32632\")\n","\n","# Search nearest ARPA sensor to ERA5 variable\n","era5_vars = [c for c in era5_summary.columns if \"_current\" in c or \"_previous\" in c]\n","\n","for var in era5_vars:\n","    nearest_idx = era5_gdf.sindex.nearest(summary_gdf.geometry, return_all=False)[1]\n","    summary_gdf[var] = era5_gdf.iloc[nearest_idx][var].values\n","\n","# New WGS84\n","summary_gdf = summary_gdf.to_crs(\"EPSG:4326\")\n","\n","# ERA5 + ARPA\n","print(f\"\\nARPA sensors enriched with ERA5 variables for {state['pollutant']}\"\n","      f\"from {cfg['start_date'].date()} to {cfg['end_date'].date()} \"\n","      f\"in {state['province']}\\nDataset contains {len(summary_gdf)} registers\\n\")\n","display(summary_gdf.head(10))\n","summary_gdf[\"date\"].value_counts().sort_index()"]},{"cell_type":"markdown","id":"b8f05884-bec0-4491-9a47-9a4b0e9ee816","metadata":{"id":"b8f05884-bec0-4491-9a47-9a4b0e9ee816"},"source":["# SENTINEL-5P"]},{"cell_type":"markdown","id":"02024a0c-948f-4dfb-81b2-477272078518","metadata":{"id":"02024a0c-948f-4dfb-81b2-477272078518"},"source":["## Sentinel 5P PRECONFIGURATION"]},{"cell_type":"code","execution_count":null,"id":"13e8fb80-4104-46a0-a49b-f2db4c66c37d","metadata":{"id":"13e8fb80-4104-46a0-a49b-f2db4c66c37d"},"outputs":[],"source":["# Pollutant name mapping for Sentinel\n","def normalize_name(s: str) -> str:\n","    s = unicodedata.normalize(\"NFKD\", str(s)).encode(\"ascii\", \"ignore\").decode()\n","    return re.sub(r\"[^a-z0-9]+\", \" \", s.lower()).strip()\n","\n","ALIASES = {\n","    \"no2\": {\"no2\", \"diossido di azoto\", \"biossido di azoto\", \"dioxido de nitrogeno\", \"dióxido de nitrógeno\", \"nitrogen dioxide\",\n","            \"nitrogeno dioxido\", \"ossidi di azoto\",\"NO2\", \"Diossido di Azoto\", \"Biossido di Azoto\", \"Dioxido de Nitrogeno\",\n","            \"Dióxido de Nitrógeno\", \"Nitrogen Dioxide\",\n","            \"Nitrogeno Dioxido\", \"Ossidi di Azoto\",},\n","    \"co\": {\"co\", \"monossido di carbonio\", \"monoxido de carbono\", \"carbon monoxide\",  \"monossido de carbono\", \"carbonio monossido\",\n","           \"monóxido de carbono\", \"CO\", \"Monossido di Carbonio\", \"Monoxido de Carbono\", \"Carbon Monoxide\",  \"Monossido de Carbono\",\n","           \"Carbonio Monossido\", \"Monóxido de Carbono\"},\n","    \"o3\": {\"o3\", \"ozono\", \"ozone\", \"ozono troposferico\", \"ozono totale\",\n","           \"O3\", \"Ozono\", \"Ozone\", \"Ozono Troposferico\", \"Ozono Totale\"},\n","    \"so2\": {\"so2\", \"diossido di zolfo\", \"biossido di zolfo\", \"dioxido de azufre\", \"dióxido de azufre\", \"sulfur dioxide\", \"zolfo diossido\",\n","           \"SO2\", \"Diossido di Zolfo\", \"Biossido di Zolfo\", \"Dioxido de Azufre\", \"Dióxido de Azufre\", \"Sulfur Dioxide\", \"Zolfo Diossido\"},\n","}\n","\n","LOOKUP = {normalize_name(n): k for k, names in ALIASES.items() for n in names}\n","\n","def get_pollutant_key(pollutant: str) -> str:\n","    k = LOOKUP.get(normalize_name(pollutant))\n","    if not k:\n","        raise ValueError(f\"Unexistent pollutant '{pollutant}' for Sentinel-5P.\")\n","    return k"]},{"cell_type":"markdown","id":"fc9c94f3-573e-4c0a-b512-a9a102454af3","metadata":{"id":"fc9c94f3-573e-4c0a-b512-a9a102454af3"},"source":["## SENTINEL-5P IMAGE FETCH"]},{"cell_type":"code","execution_count":null,"id":"a844368f-6aeb-4139-ad62-8c31da2890fd","metadata":{"scrolled":true,"id":"a844368f-6aeb-4139-ad62-8c31da2890fd"},"outputs":[],"source":["def extract_s5p_value(pollutant_key, aoi_gee, summary_gdf):\n","    pollutants = {\n","        \"no2\": {\"collection\": \"COPERNICUS/S5P/OFFL/L3_NO2\", \"band\": \"NO2_column_number_density\", \"unit\": \"mol/m²\", \"scale\": 5500},\n","        \"co\": {\"collection\": \"COPERNICUS/S5P/OFFL/L3_CO\", \"band\": \"CO_column_number_density\", \"unit\": \"mol/m²\", \"scale\": 5500},\n","        \"o3\": {\"collection\": \"COPERNICUS/S5P/OFFL/L3_O3\", \"band\": \"O3_column_number_density\", \"unit\": \"mol/m²\", \"scale\": 5500},\n","        \"so2\": {\"collection\": \"COPERNICUS/S5P/OFFL/L3_SO2\", \"band\": \"SO2_column_number_density\", \"unit\": \"mol/m²\", \"scale\": 5500},\n","    }\n","\n","    cfg_pollutant = pollutants[pollutant_key]\n","    dataset_id = cfg_pollutant[\"collection\"]\n","    band_name = cfg_pollutant[\"band\"]\n","\n","    all_results = []\n","\n","    for date_analysis in pd.date_range(cfg[\"start_date\"], cfg[\"end_date\"], freq=\"D\"):\n","        start_date = str(date_analysis.date())\n","        end_date = str((date_analysis + pd.Timedelta(days=1)).date())\n","        print(f\"Extracting Sentinel-5P {pollutant_key.upper()} for {start_date}\")\n","\n","        # Sentinel 5p Images\n","        s5p = (\n","            ee.ImageCollection(dataset_id)\n","            .filterDate(start_date, end_date)\n","            .filterBounds(aoi_gee)\n","            .select(band_name)\n","            .mean()\n","        )\n","\n","        df_day = summary_gdf[summary_gdf[\"date\"] == pd.to_datetime(start_date).date()].copy()\n","        if df_day.empty:\n","            print(f\"No ARPA data for {start_date}, skipping.\")\n","            continue\n","\n","        # ARPA points for extraction values\n","        def get_value(lon, lat):\n","            point = ee.Geometry.Point(lon, lat)\n","            value = s5p.reduceRegion(\n","                reducer=ee.Reducer.mean(),\n","                geometry=point,\n","                scale=cfg_pollutant[\"scale\"],\n","            ).get(band_name)\n","            return value.getInfo()\n","\n","        df_day[f\"Sentinel 5P {pollutant_key.upper()} Concentration (mol/m²)\"] = [\n","            get_value(lon, lat) for lon, lat in zip(df_day[\"lng\"], df_day[\"lat\"])\n","        ]\n","        df_day[\"Sentinel 5P Dataset\"] = dataset_id\n","        df_day[\"Sentinel 5P Band\"] = band_name\n","        df_day[\"Sentinel 5P Units\"] = cfg_pollutant[\"unit\"]\n","        df_day[\"Sentinel 5P Date\"] = start_date\n","\n","        all_results.append(df_day)\n","\n","        # Current day results\n","        print(f\"Day {start_date} processed — {len(df_day)} registers\")\n","        display(df_day.head(3))\n","\n","    if all_results:\n","        summary_with_s5p = pd.concat(all_results, ignore_index=True)\n","        print(f\"\\nSentinel Extraction Done — {len(all_results)} processed days.\")\n","        print(f\"   Dates: {cfg['start_date'].date()} → {cfg['end_date'].date()}\")\n","        print(f\"   Total registers: {len(summary_with_s5p)}\")\n","        return summary_with_s5p\n","    else:\n","        print(\"No results generated\")\n","        return summary_gdf\n","\n","pollutant_key = get_pollutant_key(pollutant.value)\n","summary_with_s5p = extract_s5p_value(pollutant_key, cfg[\"aoi_gee\"], summary_gdf)\n"]},{"cell_type":"markdown","id":"75d151dc-9392-40aa-8fb9-c1fa34fd6cc3","metadata":{"id":"75d151dc-9392-40aa-8fb9-c1fa34fd6cc3"},"source":["# Enhanced Visualization"]},{"cell_type":"markdown","id":"63e115c5-ea0e-4612-b1b0-f135d196cabc","metadata":{"id":"63e115c5-ea0e-4612-b1b0-f135d196cabc"},"source":["### Rename and Reorder"]},{"cell_type":"code","execution_count":null,"id":"c90fb36e-5d87-4ed2-a41d-74b836fe1541","metadata":{"id":"c90fb36e-5d87-4ed2-a41d-74b836fe1541"},"outputs":[],"source":["# Tags\n","pollutant_symbols = {\n","    \"Monossido di Carbonio\": \"CO\",\n","    \"Diossido di Azoto\": \"NO₂\",\n","    \"Ozono\": \"O₃\",\n","    \"Diossido di Zolfo\": \"SO₂\"\n","}\n","\n","# Pollutant chosen by user\n","selected_pollutant = state[\"pollutant\"]\n","pollutant_symbol = pollutant_symbols.get(selected_pollutant, selected_pollutant)\n","pollutant_key = get_pollutant_key(selected_pollutant)\n","\n","# Renaming map\n","rename_cols = {\n","    \"date\": \"Date\",\n","    \"provincia\": \"Province\",\n","    \"pollutant\": \"Pollutant\",\n","    \"idsensore\": \"ID Sensor\",\n","    \"lat\": \"Latitude\",\n","    \"lng\": \"Longitude\",\n","    \"curr_mean\": f\"Current Mean Concentration ({pollutant_symbol}, µg/m³)\",\n","    \"prev_mean\": f\"Previous Mean Concentration ({pollutant_symbol}, µg/m³)\",\n","    \"t2m_c_current\": \"Current Temperature (°C)\",\n","    \"t2m_c_previous\": \"Previous Temperature (°C)\",\n","    \"sp_hpa_current\": \"Current Pressure (hPa)\",\n","    \"sp_hpa_previous\": \"Previous Pressure (hPa)\",\n","    \"tp_current\": \"Current Total Precipitation (mm)\",\n","    \"tp_previous\": \"Previous Total Precipitation (mm)\",\n","    \"wind_speed_current\": \"Current Windspeed (m/s)\",\n","    \"wind_speed_previous\": \"Previous Windspeed (m/s)\",\n","    \"ssr_kJ_current\": \"Current Surface Solar Radiation (kJ/m²)\",\n","    \"ssr_kJ_previous\": \"Previous Surface Solar Radiation (kJ/m²)\",\n","    \"str_kJ_current\": \"Current Surface Thermal Radiation (kJ/m²)\",\n","    \"str_kJ_previous\": \"Previous Surface Thermal Radiation (kJ/m²)\",\n","    \"blh_km_current\": \"Current Boundary Layer Height [km]\",\n","    \"blh_km_previous\": \"Previous Boundary Layer Height [km]\",\n","    f\"Sentinel 5P {pollutant_key.upper()} Concentration (mol/m²)\": f\"Sentinel 5P Concentration {pollutant_symbol} (mol/m²)\",\n","    \"Sentinel 5P Dataset\": \"Sentinel 5P Dataset\",\n","    \"Sentinel 5P Band\": \"Sentinel 5P Band\",\n","    \"Sentinel 5P Units\": \"Sentinel 5P Units\",\n","    \"Sentinel 5P Date\": \"Sentinel 5P Date\"\n","}\n","\n","cols_to_drop = [\n","    \"geometry\",\n","    \"Sentinel 5P Band\",\n","    \"Sentinel 5P Units\",\n","    \"Sentinel 5P Dataset\",\n","    \"Sentinel 5P Date\"\n","]\n","\n","#  Desired order\n","order = [\n","    \"Date\", \"Province\", \"ID Sensor\", \"Latitude\", \"Longitude\", \"Pollutant\",\n","    f\"Previous Mean Concentration ({pollutant_symbol}, µg/m³)\",\n","    f\"Current Mean Concentration ({pollutant_symbol}, µg/m³)\",\n","    f\"Sentinel 5P Concentration {pollutant_symbol} (mol/m²)\",\n","    \"Sentinel 5P Date\",\n","    \"Previous Temperature (°C)\", \"Current Temperature (°C)\",\n","    \"Previous Pressure (hPa)\", \"Current Pressure (hPa)\",\n","    \"Previous Total Precipitation (mm)\", \"Current Total Precipitation (mm)\",\n","    \"Previous Windspeed (m/s)\", \"Current Windspeed (m/s)\",\n","    \"Previous Surface Solar Radiation (kJ/m²)\", \"Current Surface Solar Radiation (kJ/m²)\",\n","    \"Previous Surface Thermal Radiation (kJ/m²)\", \"Current Surface Thermal Radiation (kJ/m²)\",\n","    \"Previous Boundary Layer Height [km]\",\"Current Boundary Layer Height [km]\",\n","]\n","\n","# Apply renaming and  reorder\n","summary = summary_with_s5p.rename(columns=rename_cols)\n","\n","summary = summary[\n","    [c for c in order if c in summary.columns] +\n","    [c for c in summary.columns if c not in order]\n","]\n","summary = summary.drop(columns=[c for c in cols_to_drop if c in summary.columns], errors=\"ignore\")\n","\n","\n","print(f\"\\nFinal dataset ready: {len(summary)} records, {len(summary.columns)} columns.\")"]},{"cell_type":"markdown","id":"074532b4-6ec4-42b2-a8ad-bc217970fe64","metadata":{"id":"074532b4-6ec4-42b2-a8ad-bc217970fe64"},"source":["## Visualization of data with NaN"]},{"cell_type":"code","execution_count":null,"id":"7d54df31-a756-44e2-a597-cdb25dbd5fe4","metadata":{"id":"7d54df31-a756-44e2-a597-cdb25dbd5fe4"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","import pandas as pd\n","\n","# Cleaning\n","summary = summary.loc[:, ~summary.columns.duplicated()]\n","summary.columns.name = None\n","summary = summary.round(10)\n","\n","# Column grouping by keyword\n","cols_concentration = [c for c in summary.columns if \"Concentration\" in c]\n","cols_sentinel      = [c for c in summary.columns if \"Sentinel\" in c]\n","cols_precipitacion = [c for c in summary.columns if \"Precipitation\" in c]\n","cols_temperature   = [c for c in summary.columns if \"Temperature\" in c]\n","cols_pressure      = [c for c in summary.columns if \"Pressure\" in c]\n","cols_wind          = [c for c in summary.columns if \"Windspeed\" in c]\n","cols_blh           = [c for c in summary.columns if \"Boundary Layer\" in c]\n","cols_surface       = [c for c in summary.columns if \"Surface\" in c]\n","\n","# Ensure numeric\n","for cols in [cols_concentration, cols_sentinel, cols_precipitacion, cols_temperature,\n","             cols_pressure, cols_wind, cols_blh, cols_surface]:\n","    for c in cols:\n","        summary[c] = pd.to_numeric(summary[c], errors=\"coerce\")\n","\n","def get_col_scale(series, buffer=0.05):\n","    vals = pd.to_numeric(series, errors=\"coerce\").dropna()\n","    if len(vals) == 0:\n","        return (None, None)\n","    vmin, vmax = vals.min(), vals.max()\n","    diff = vmax - vmin\n","    return (vmin - diff*buffer, vmax + diff*buffer)\n","\n","# Dynamic scales computation for all columns\n","scales = {}\n","for c in summary.select_dtypes(include=[\"float\", \"int\"]).columns:\n","    vmin, vmax = get_col_scale(summary[c])\n","    if vmin is not None and vmax is not None:\n","        scales[c] = (vmin, vmax)\n","\n","# Palettes\n","color_map_rules = {\n","    \"Concentration\": \"YlOrRd\",\n","    \"Sentinel\": \"YlOrRd\",\n","    \"Precipitation\": \"GnBu\",\n","    \"Temperature\": \"coolwarm\",\n","    \"Surface\": \"hot\",\n","    \"Pressure\": \"Blues\",\n","    \"Windspeed\": \"Purples\",\n","    \"Boundary Layer\": \"Greys\"\n","}\n","\n","# Styling\n","styled_summary = summary.style\n","\n","for col in summary.columns:\n","    cmap = None\n","    for key, cmap_name in color_map_rules.items():\n","        if key in col:\n","            cmap = cmap_name\n","            break\n","    if cmap and col in scales:\n","        vmin, vmax = scales[col]\n","        styled_summary = styled_summary.background_gradient(\n","            subset=[col], cmap=cmap, vmin=vmin, vmax=vmax\n","        )\n","\n","styled_summary = (\n","    styled_summary\n","    .set_properties(**{\n","        \"text-align\": \"center\",\n","        \"font-family\": \"Segoe UI, sans-serif\",\n","        \"font-size\": \"10pt\",\n","        \"border\": \"1px solid #ddd\",\n","        \"padding\": \"3px\"\n","    })\n","    .format(precision=6)\n",")\n","\n","display(styled_summary)\n","\n","# Legends\n","legend_data = {}\n","for key, cmap_name in color_map_rules.items():\n","    cols = [c for c in summary.columns if key in c and c in scales]\n","    if cols:\n","        vmins = [scales[c][0] for c in cols if scales[c][0] is not None]\n","        vmaxs = [scales[c][1] for c in cols if scales[c][1] is not None]\n","        if vmins and vmaxs:\n","            legend_data[key] = (cmap_name, min(vmins), max(vmaxs))\n","\n","fig, axes = plt.subplots(1, len(legend_data), figsize=(3*len(legend_data), 1))\n","if len(legend_data) == 1:\n","    axes = [axes]\n","\n","for ax, (label, (cmap_name, vmin, vmax)) in zip(axes, legend_data.items()):\n","    if vmin is not None and vmax is not None and vmin != vmax:\n","        norm = mpl.colors.Normalize(vmin=vmin, vmax=vmax)\n","        cmap = mpl.cm.get_cmap(cmap_name)\n","        mpl.colorbar.ColorbarBase(ax, cmap=cmap, norm=norm, orientation=\"horizontal\")\n","    ax.set_title(label, fontsize=8)\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","id":"663b3ce0-4c19-4094-ae6d-83ec58056c5f","metadata":{"id":"663b3ce0-4c19-4094-ae6d-83ec58056c5f"},"source":["## Visualization of data WITHOUT NaN"]},{"cell_type":"code","execution_count":null,"id":"d4e4cb9b-89a6-42a5-acbf-c50597056bd3","metadata":{"id":"d4e4cb9b-89a6-42a5-acbf-c50597056bd3"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","import pandas as pd\n","import numpy as np\n","\n","# Cleaning\n","summary = summary.loc[:, ~summary.columns.duplicated()]\n","summary.columns.name = None\n","summary = summary.replace([\"None\", \"none\", \"NaN\", \"nan\", \"\"], np.nan)\n","summary = summary.round(10)\n","\n","# Convert columns to numeric when possible\n","for col in summary.columns:\n","    try:\n","        summary[col] = pd.to_numeric(summary[col])\n","    except (ValueError, TypeError):\n","        pass\n","\n","# Count and drop NaN rows\n","nan_before = len(summary)\n","summary = summary.dropna(how=\"all\").reset_index(drop=True)\n","nan_removed = nan_before - len(summary)\n","\n","# Dynamic Selection by keyword\n","cols_concentration = [c for c in summary.columns if \"Concentration\" in c]\n","cols_sentinel      = [c for c in summary.columns if \"Sentinel\" in c]\n","cols_precipitacion = [c for c in summary.columns if \"Precipitation\" in c]\n","cols_temperature   = [c for c in summary.columns if \"Temperature\" in c]\n","cols_pressure      = [c for c in summary.columns if \"Pressure\" in c]\n","cols_wind          = [c for c in summary.columns if \"Windspeed\" in c]\n","cols_blh           = [c for c in summary.columns if \"Boundary Layer\" in c]\n","cols_surface       = [c for c in summary.columns if \"Surface\" in c]\n","\n","# Negative values removal for concentration columns only\n","neg_before = len(summary)\n","if cols_concentration:\n","    neg_count = (summary[cols_concentration] < 0).sum().sum()\n","    summary = summary[(summary[cols_concentration] >= 0).all(axis=1)]\n","    neg_removed = neg_before - len(summary)\n","else:\n","    neg_count = 0\n","    neg_removed = 0\n","\n","#  Cleaning summary printout\n","print(f\" Cleaning summary:\")\n","print(f\"   • NaN rows removed: {nan_removed}\")\n","print(f\"   • Rows removed for negative concentrations: {neg_removed} ({int(neg_count)} total negative values)\")\n","print(f\"   • Final dataset size: {len(summary)} rows × {len(summary.columns)} columns\\n\")\n","\n","# Function for dynamic min/max per column\n","def get_col_scale(series, buffer=0.05):\n","    vals = pd.to_numeric(series, errors=\"coerce\").dropna()\n","    if len(vals) == 0:\n","        return (None, None)\n","    vmin, vmax = vals.min(), vals.max()\n","    diff = vmax - vmin\n","    return (vmin - diff * buffer, vmax + diff * buffer)\n","\n","# Compute dynamic scales per column\n","scales = {}\n","for c in summary.select_dtypes(include=[\"float\", \"int\"]).columns:\n","    vmin, vmax = get_col_scale(summary[c])\n","    if vmin is not None and vmax is not None:\n","        scales[c] = (vmin, vmax)\n","\n","# Palettes\n","color_map_rules = {\n","    \"Concentration\": \"YlOrRd\",\n","    \"Sentinel\": \"YlOrRd\",\n","    \"Precipitation\": \"GnBu\",\n","    \"Temperature\": \"coolwarm\",\n","    \"Surface\": \"hot\",\n","    \"Pressure\": \"Blues\",\n","    \"Windspeed\": \"Purples\",\n","    \"Boundary Layer\": \"Greys\"\n","}\n","\n","# Dynamic styling\n","styled_summary = summary.style\n","for col in summary.columns:\n","    cmap = None\n","    for key, cmap_name in color_map_rules.items():\n","        if key in col:\n","            cmap = cmap_name\n","            break\n","    if cmap and col in scales:\n","        vmin, vmax = scales[col]\n","        styled_summary = styled_summary.background_gradient(\n","            subset=[col], cmap=cmap, vmin=vmin, vmax=vmax\n","        )\n","\n","# Global style\n","styled_summary = (\n","    styled_summary\n","    .set_properties(**{\n","        \"text-align\": \"center\",\n","        \"font-family\": \"Segoe UI, sans-serif\",\n","        \"font-size\": \"10pt\",\n","        \"border\": \"1px solid #ddd\",\n","        \"padding\": \"3px\"\n","    })\n","    .format(precision=6)\n",")\n","\n","display(styled_summary)\n","\n","# Legend\n","color_maps = {}\n","for key, cmap_name in color_map_rules.items():\n","    cols = [c for c in summary.columns if key in c]\n","    if cols:\n","        vmin = summary[cols].min().min()\n","        vmax = summary[cols].max().max()\n","        color_maps[f\"{key} ({cmap_name})\"] = (cmap_name, vmin, vmax)\n","\n","fig, axes = plt.subplots(1, len(color_maps), figsize=(3 * len(color_maps), 1))\n","if len(color_maps) == 1:\n","    axes = [axes]\n","\n","for ax, (label, (cmap_name, vmin, vmax)) in zip(axes, color_maps.items()):\n","    if pd.notna(vmin) and pd.notna(vmax) and vmin != vmax:\n","        norm = mpl.colors.Normalize(vmin=vmin, vmax=vmax)\n","        cmap = mpl.cm.get_cmap(cmap_name)\n","        mpl.colorbar.ColorbarBase(ax, cmap=cmap, norm=norm, orientation=\"horizontal\")\n","    ax.set_title(label, fontsize=8)\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"id":"2c0a3037-5da6-45f2-a015-84233f52e952","metadata":{"id":"2c0a3037-5da6-45f2-a015-84233f52e952"},"outputs":[],"source":["start_str = pd.to_datetime(cfg[\"start_date\"]).strftime(\"%Y%m%d\")\n","end_str = pd.to_datetime(cfg[\"end_date\"]).strftime(\"%Y%m%d\")\n","\n","# Folder creation\n","output_dir = f\"results/ARPA_ERA_S5P-{start_str}_to_{end_str}\"\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# CSV file output\n","output_path = os.path.join(output_dir, \"summary_clean.csv\")\n","\n","\n","summary.to_csv(output_path, index=False)\n","\n","print(f\"\\nFile saved in:\\n{output_path}\\n\")\n"]}],"metadata":{"kernelspec":{"display_name":"Python (cmcc_env)","language":"python","name":"cmcc_env"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.15"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}